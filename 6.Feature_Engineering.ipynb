{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c5e2cf0-0907-4d17-aaa5-8ad7b7772f42",
   "metadata": {},
   "source": [
    "# __Features and Feature Engineering__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c1517c-7d5c-4177-beb8-c1720637c7e8",
   "metadata": {},
   "source": [
    "__Features__ represent the transformed raw data.A feature—also called a dimension—is an input variable used to generate model predictions. \n",
    "__Feature Engineering__ preprocesses raw data into a machine-readable format. It optimizes ML model performance by transforming and selecting relevant features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fdabe5-47c1-433f-9c60-304c29293ca0",
   "metadata": {},
   "source": [
    "#### __Dimensionality Reduction__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7b3276-5fb1-46c4-bfae-33b18aa2c388",
   "metadata": {},
   "source": [
    "It can be described as a combination of both feature selection and feature extraction. Essentially we can lower the data dimensionality using both techniques. \n",
    "\n",
    "- When applying feature selection, working with a subset of original data indicates that we will have fewer features compared to the original dataset.\n",
    "- When applying feature extraction, although the no. of newly constructed features might be the same as the original, we usually use only the significant ones.\n",
    "- In particular, evaluating the retained variants in the dataset helps us select those features that preserve the most information for our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1e9555-d552-4977-be5d-c609052e000f",
   "metadata": {},
   "source": [
    "## __Feature engineering techniques__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9ec28c-2d87-42cf-8b11-168f899a145d",
   "metadata": {},
   "source": [
    "### __Feature transformation__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaae77ca-ec03-4ac9-9bff-843897355b49",
   "metadata": {},
   "source": [
    "Feature transformation is the process of converting one feature type into another, more readable form for a particular model. This consists of transforming continuous into categorical data, or vice-versa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3f143f-2468-418c-b76e-4b8007882fe7",
   "metadata": {},
   "source": [
    "#### __Binning__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13a9697-459e-4753-b8b3-79cd7ab07959",
   "metadata": {},
   "source": [
    "This technique essentially transforms continuous, numerical values into categorical features. Specifically, binning compares each value to the neighborhood of values surrounding it and then sorts data points into a number of bins. A rudimentary example of binning is age demographics, in which continuous ages are divided into age groups.\n",
    "\n",
    "- Once values have been placed into bins, one can further smooth the bins by means, medians or boundaries.\n",
    "- Smoothing bins replaces a bin’s contained values with bin-derived values.\n",
    "- Binning creates categorical values from continuous ones.\n",
    "- Smoothing bins is a form of local smoothing meant to reduce noise in input data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba291382-7b6b-4128-b338-d859f6f6cb1d",
   "metadata": {},
   "source": [
    "#### __One-hot encoding__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2952a91-5c14-4dd4-81c5-583ca26abcb4",
   "metadata": {},
   "source": [
    "This is the inverse of binning; it creates numerical features from categorical variables. One-hot encoding maps categorical features to binary representations, which are used to map the feature in a matrix or vector space. \n",
    "\n",
    "- Bag of words models are an example of one-hot encoding frequently used in natural language processing tasks.\n",
    "- Another example of one-hot encoding is spam filtering classification in which the categories spam and not spam are converted to 1 and 0 respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96a1b8e-dbf9-41d5-82ef-e1448c15bdc2",
   "metadata": {},
   "source": [
    "#### __Feature Selection__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00af2bf9-deda-4edf-a54d-03f6e553af65",
   "metadata": {},
   "source": [
    "Feature selection is the process of reducing the number of features used in various types of machine learning models, in a way that leaves the predictive\n",
    "ability of the data preserved. It is the processing of selecting a subset of variables in order to create a new model with the purpose of reducing multicollinearity, and so maximize model generalizability and optimization.\n",
    "\n",
    "- When dealing with real-world data, we can encounter hundreds or thousands of features that exceed the no. of observations. This leads to a high-dimensional dataset.\n",
    "- A set with many features probably means that the volume is large but the data points are very few and far apart.\n",
    "- This can impact the performance of the machine learning algorithm, making it unreliable.\n",
    "- In such cases, we can try to remove the irrelevant features without losing essential information from the dataset.\n",
    "- Hence, reducing the features can boost the training accuracy of the model, hence its predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5150f5ae-a526-428e-8760-e1ab4f15e52c",
   "metadata": {},
   "source": [
    "#### __Feature Extraction__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf28646f-a114-408d-baf2-e443745b23c4",
   "metadata": {},
   "source": [
    "Feature Extraction is the process of transforming existing features into new ones. In essence, we are not using a subset of the original features. Instead, we create new ones, a linear combination of the originals. Here, also the main goal is to capture the same information with fewer features.\n",
    "\n",
    "The difference between __feature selection__ and __feature extraction__ is that the first technique retains a subset of the original features while feature extraction generates new features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b5b468-b813-4ef3-9e10-ffba35ad322d",
   "metadata": {},
   "source": [
    "### __Feature scaling__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f025dfda-0f4e-4967-a2ab-c907d46ef9e1",
   "metadata": {},
   "source": [
    "Certain features have upper and lower bounds intrinsic to data that limits possible feature values, such as time-series data or age. But in many cases, model features may not have a limitation on possible values, and such large feature scales (being the difference between a features lowest and highest values) can negatively affect certain models. \n",
    "\n",
    "- Feature scaling (sometimes called feature normalization) is a standardization technique to rescale features and limit the impact of large scales on models.12\n",
    "- While feature transformation transforms data from one type to another, feature scaling transforms data in terms of range and distribution, maintaining its original data type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b1e826-323e-4d7a-8eb7-d155b6b539d4",
   "metadata": {},
   "source": [
    "#### __Min-max scaling__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81387712-04cd-4bd6-9788-e88946e6a4d7",
   "metadata": {},
   "source": [
    "Min-max scaling rescales all values for a given feature so that they fall between specified minimum and maximum values, often 0 and 1. Each data point’s value for the selected feature (represented by x) is computed against the decided minimum and maximum feature values, min(x) and max(x) respectively, which produces the new feature value for that data point (represented by $x̃$ ).\n",
    "\n",
    "<img src=\"https://www.ibm.com/content/dam/connectedassets-adobe-cms/worldwide-content/creative-assets/s-migr/ul/g/e4/b2/screen2.jpg\" width=250 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478872cd-1b02-4aa7-80cb-6aad2cc32419",
   "metadata": {},
   "source": [
    "#### __Z-score scaling__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4ecdf4-73bf-4b1b-9c79-8fa7b012b1c5",
   "metadata": {},
   "source": [
    "Also known as standardization and variance scaling. Whereas min-max scaling scales feature values to fit within designated minimum and maximum values, z-score scaling rescales features so that they have a shared standard deviation of 1 with a mean of 0.\n",
    "\n",
    "<img src=\"https://www.ibm.com/content/dam/connectedassets-adobe-cms/worldwide-content/creative-assets/s-migr/ul/g/b4/c0/screen1.jpg\" width=250 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7a8a0b-ec99-4971-9f6d-051fc76cf5ec",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a73a9807-66cd-4668-b45c-f918ec1d4e10",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

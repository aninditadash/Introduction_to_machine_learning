{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q788pGwAJCPp"
   },
   "source": [
    "# __Sklearn Pipelines__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NXsPrWcaJCPp"
   },
   "source": [
    "## __Why sklearn pipelines?__\n",
    "\n",
    "**Pipelines provide an organized approach to managing your data preprocessing and modeling code. They combine preprocessing and modeling steps into a single, streamlined process.**\n",
    "\n",
    " - **Cleaner Code**: Pipelines eliminate the need to manually manage training and validation data at each preprocessing step, reducing clutter and complexity.\n",
    "\n",
    " - **Fewer Bugs**: By bundling steps together, pipelines minimize the risk of misapplying or forgetting a preprocessing step.\n",
    "\n",
    " - **Easier to Productionize**: Pipelines simplify the transition from a prototype model to a scalable, deployable solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddQRwBIUJCPp"
   },
   "source": [
    "**Syntax**:\n",
    "```python\n",
    "class sklearn.pipeline.Pipeline(steps, *, memory=None, verbose=False)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eu7-ItBuJCPp"
   },
   "source": [
    "### __Before we start the application, let's pay attention to the following important points:__\n",
    "\n",
    " - A pipeline is a sequence of data transformers that can include a final predictor.\n",
    "\n",
    " - It lets you apply multiple preprocessing steps to your data in order, and optionally end with a predictor for modeling.\n",
    "\n",
    " - Each intermediate step in the pipeline must have fit and transform methods, while the final step only needs fit.\n",
    "\n",
    " - You can cache these transformers using the memory argument.\n",
    "\n",
    " - The pipeline's main goal is to combine multiple steps that can be cross-validated together and have their parameters adjusted.\n",
    "\n",
    " - You can set parameters for any step by using its name followed by a double underscore(__) and the parameter name.\n",
    "\n",
    " - You can replace any step's estimator with another estimator or remove a transformer by setting it to 'passthrough' or None."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7PN4_6toJCPp"
   },
   "source": [
    "### Lets look at the housing data with `Ocean proximity` feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "id": "uHc7WIJsJCPp"
   },
   "outputs": [],
   "source": [
    "housing_data = pd.read_csv(\"housing_with_ocean_proximity.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "id": "dY929xeLJCPp",
    "outputId": "7f8eda64-4444-4f40-ae4e-9763a1f0aff8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>358500.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-122.24</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1467.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>7.2574</td>\n",
       "      <td>352100.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>5.6431</td>\n",
       "      <td>341300.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>3.8462</td>\n",
       "      <td>342200.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.23     37.88                41.0        880.0           129.0   \n",
       "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
       "2    -122.24     37.85                52.0       1467.0           190.0   \n",
       "3    -122.25     37.85                52.0       1274.0           235.0   \n",
       "4    -122.25     37.85                52.0       1627.0           280.0   \n",
       "\n",
       "   population  households  median_income  median_house_value ocean_proximity  \n",
       "0       322.0       126.0         8.3252            452600.0        NEAR BAY  \n",
       "1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  \n",
       "2       496.0       177.0         7.2574            352100.0        NEAR BAY  \n",
       "3       558.0       219.0         5.6431            341300.0        NEAR BAY  \n",
       "4       565.0       259.0         3.8462            342200.0        NEAR BAY  "
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "id": "aFxYo8Pw5JrC"
   },
   "outputs": [],
   "source": [
    "# Separate features and target variable\n",
    "# Assuming 'median_house_value' is the target variable in your dataset\n",
    "\n",
    "X = housing_data.drop('median_house_value', axis=1)\n",
    "y = housing_data['median_house_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {
    "id": "i0H6lIExJCPp"
   },
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "id": "PmeWXusI5JrC",
    "outputId": "c5a47564-f778-4cec-bf5b-e7ed72820fbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 14448 entries, 4722 to 7816\n",
      "Data columns (total 9 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   longitude           14448 non-null  float64\n",
      " 1   latitude            14448 non-null  float64\n",
      " 2   housing_median_age  14448 non-null  float64\n",
      " 3   total_rooms         14448 non-null  float64\n",
      " 4   total_bedrooms      14286 non-null  float64\n",
      " 5   population          14448 non-null  float64\n",
      " 6   households          14448 non-null  float64\n",
      " 7   median_income       14448 non-null  float64\n",
      " 8   ocean_proximity     14448 non-null  object \n",
      "dtypes: float64(8), object(1)\n",
      "memory usage: 1.1+ MB\n",
      "None \n",
      "\n",
      "longitude               0\n",
      "latitude                0\n",
      "housing_median_age      0\n",
      "total_rooms             0\n",
      "total_bedrooms        162\n",
      "population              0\n",
      "households              0\n",
      "median_income           0\n",
      "ocean_proximity         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print info to check the structure of the training data\n",
    "\n",
    "print(X_train.info(), \"\\n\")\n",
    "print(X_train.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qKqV5G9sJCPp"
   },
   "source": [
    "#### __Implementation for sklearn pipelines__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_tuxhnoYJCPp"
   },
   "source": [
    "![link text](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/ML/Lesson_03/Sklearn_pipeline_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_nY3QOuJCPp"
   },
   "source": [
    "![link text](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/ML/Lesson_03/Sklearn_pipeline_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FdtR51LVJCPp"
   },
   "source": [
    "#### __We need to perform following preprocessing steps before building the model__|\n",
    "\n",
    "1. Missing value treatment - 162 missing values in total_bedrooms(a numeric column)\n",
    "2. Dummy variable creation for categorical data\n",
    "3. Standardization of numeric variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {
    "id": "6XnT3bUdJCPq"
   },
   "outputs": [],
   "source": [
    "# import StandardScaler for standardization and OneHotEncoder for creating dummy variables\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# import SimpleImputer for missing value treatment\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# importing pipeline class. The Pipeline class is used to create a sequence of data processing steps.\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# importing ColumnTransformer class to apply different preprocessing steps to different subsets of features in your dataset.\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjXlZ3znJCPq"
   },
   "source": [
    "##### __A note about ColumnTransformer__\n",
    "\n",
    "* ColumnTransformer class allows you to apply different preprocessing steps to different subsets of features in your dataset.\n",
    "* This is particularly useful when you have a mix of numerical and categorical data that require different types of preprocessing.\n",
    "* ColumnTransformer ensures that each column or group of columns gets the appropriate transformation before combining the results for further processing or modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sLpxNEFaJCPq"
   },
   "source": [
    "### __Data preprocessing starts here:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBrCwrleJCPq"
   },
   "source": [
    "#### Steps to perform:\n",
    "    \n",
    "#### Step 1: Let's store the names of numeric and object type variables separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {
    "id": "e6jD0wYsJCPq"
   },
   "outputs": [],
   "source": [
    "# Set up preprocessing steps for numeric and categorical data\n",
    "\n",
    "housing_cat = X_train.select_dtypes(include='object').columns\n",
    "housing_num = X_train.select_dtypes(exclude='object').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ocean_proximity'], dtype='object')"
      ]
     },
     "execution_count": 545,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n",
       "       'total_bedrooms', 'population', 'households', 'median_income'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mkgemfJ-JCPq"
   },
   "source": [
    "------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xyDCfq3lJCPq"
   },
   "source": [
    "#### Step 2: Set-up sklearn pipeline for numeric variables. We need to perform missing value imputation and then standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {
    "id": "KudVlvB3JCPq"
   },
   "outputs": [],
   "source": [
    "# Numeric variables pipeline\n",
    "# Here, null values will be imputed with the median value of the dataset\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('std_scaler', StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric variables pipeline\n",
    "# If we want to remove the rows with the null values (NOT WORKING)\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value=np.nan)),\n",
    "    ('std_scaler', StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical variables pipeline\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('one-hot-encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Bj7McC2JCPq"
   },
   "source": [
    "The `num_pipeline` is a pipeline that preprocesses numerical data in two steps:\n",
    "\n",
    "- Imputation: Fills missing values using the median value of each column (SimpleImputer(strategy='median')).\n",
    "- Standardization: Scales the data to have a mean of 0 and a standard deviation of 1 (StandardScaler()).\n",
    "\n",
    "This pipeline ensures consistent and streamlined preprocessing of numerical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-ch2NFiJCPq"
   },
   "source": [
    "------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7TuFRTCkJCPq"
   },
   "source": [
    "#### Step 3: Unified Data Preprocessing with Pipelines and ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {
    "id": "aQR14gk_JCPq"
   },
   "outputs": [],
   "source": [
    "# Unified preprocessing for both numeric and categorical data\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "    ('num', num_pipeline, housing_num),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore'), housing_cat)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unified preprocessing for both numeric and categorical data\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "    ('num', num_pipeline, housing_num),\n",
    "    ('cat', cat_pipeline, housing_cat)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-A3phf1JCPq"
   },
   "source": [
    "The preprocessing step uses ColumnTransformer to apply different preprocessing pipelines to different types of data in the dataset:\n",
    "\n",
    " - Numerical Data: Applies num_pipeline to columns in housing_num, performing imputation and standardization.\n",
    " - Categorical Data: Applies OneHotEncoder to columns in housing_cat, converting categorical variables into a one-hot encoded format, ignoring unknown categories.\n",
    "\n",
    "This ensures that both numerical and categorical data are preprocessed appropriately within a single, unified framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yaI1Qty4JCPq"
   },
   "source": [
    "------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XQ0EBrXVJCPq"
   },
   "source": [
    "#### Let's check how the pipeline we have created works with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {
    "id": "rX9c4geYJCPq"
   },
   "outputs": [],
   "source": [
    "# applying preprocessing pipeline to train data\n",
    "\n",
    "check_train = preprocessing.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {
    "id": "9t80hzsdJCPq",
    "outputId": "9eca293f-8b46-4968-b031-3b7ce1d59733"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.604388</td>\n",
       "      <td>-0.741523</td>\n",
       "      <td>1.536729</td>\n",
       "      <td>-0.620001</td>\n",
       "      <td>-0.715386</td>\n",
       "      <td>-0.790880</td>\n",
       "      <td>-0.721514</td>\n",
       "      <td>0.072853</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.798753</td>\n",
       "      <td>-0.862998</td>\n",
       "      <td>0.501005</td>\n",
       "      <td>-0.119890</td>\n",
       "      <td>-0.133529</td>\n",
       "      <td>0.218924</td>\n",
       "      <td>-0.043175</td>\n",
       "      <td>0.129574</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.845868</td>\n",
       "      <td>1.445014</td>\n",
       "      <td>-1.809456</td>\n",
       "      <td>0.736722</td>\n",
       "      <td>0.329095</td>\n",
       "      <td>0.298266</td>\n",
       "      <td>0.353837</td>\n",
       "      <td>0.825943</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.279450</td>\n",
       "      <td>0.828299</td>\n",
       "      <td>-1.092417</td>\n",
       "      <td>-0.018491</td>\n",
       "      <td>-0.414919</td>\n",
       "      <td>-0.394171</td>\n",
       "      <td>-0.348165</td>\n",
       "      <td>3.519281</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.237318</td>\n",
       "      <td>-1.381600</td>\n",
       "      <td>1.457057</td>\n",
       "      <td>-0.847115</td>\n",
       "      <td>-0.739232</td>\n",
       "      <td>-0.702522</td>\n",
       "      <td>-0.747807</td>\n",
       "      <td>-0.986694</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.604388 -0.741523  1.536729 -0.620001 -0.715386 -0.790880 -0.721514   \n",
       "1  0.798753 -0.862998  0.501005 -0.119890 -0.133529  0.218924 -0.043175   \n",
       "2 -0.845868  1.445014 -1.809456  0.736722  0.329095  0.298266  0.353837   \n",
       "3 -1.279450  0.828299 -1.092417 -0.018491 -0.414919 -0.394171 -0.348165   \n",
       "4  1.237318 -1.381600  1.457057 -0.847115 -0.739232 -0.702522 -0.747807   \n",
       "\n",
       "         7    8    9    10   11   12  \n",
       "0  0.072853  1.0  0.0  0.0  0.0  0.0  \n",
       "1  0.129574  1.0  0.0  0.0  0.0  0.0  \n",
       "2  0.825943  0.0  1.0  0.0  0.0  0.0  \n",
       "3  3.519281  0.0  0.0  0.0  1.0  0.0  \n",
       "4 -0.986694  0.0  0.0  0.0  0.0  1.0  "
      ]
     },
     "execution_count": 565,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting array to dataframe to have a better look at it\n",
    "\n",
    "check_train_df = pd.DataFrame(check_train)\n",
    "check_train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {
    "id": "FCQZ6iNgJCPq",
    "outputId": "f5d2ea66-b060-4fb9-e684-c9aaf70f836e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       0\n",
       "3       0\n",
       "4     162\n",
       "5       0\n",
       "6       0\n",
       "7       0\n",
       "8       0\n",
       "9       0\n",
       "10      0\n",
       "11      0\n",
       "12      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 567,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No NULL values found after pre-processing\n",
    "\n",
    "check_train_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkLLRW9-JCPq"
   },
   "source": [
    "#### __Observation:__\n",
    "\n",
    "1. Good to see all the missing values are treated\n",
    "2. numeric variables are standardised\n",
    "3. ocean_proximity is converted to dummy variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_03hcT-RJCPr"
   },
   "source": [
    "------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "156uciNUJCPr"
   },
   "source": [
    "#### Step 4: Building Linear regression model and hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {
    "id": "XD6Lk-FQJCPr"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {
    "id": "eDKVzBGqJCPr"
   },
   "outputs": [],
   "source": [
    "# Create an instance of the Ridge regression model\n",
    "\n",
    "model = Ridge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {
    "id": "Lk6a6jk1JCPr"
   },
   "outputs": [],
   "source": [
    "# Build a Ridge regression model within a complete pipeline\n",
    "\n",
    "final_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessing),\n",
    "    ('model_ridge', model)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {
    "id": "rhILfy3zJCPr"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_ridge__alpha': array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. , 1.1, 1.2, 1.3,\n",
       "        1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2. ])}"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the grid of hyperparameters to search\n",
    "# Note: You can set parameters for any step by using its name followed by a double underscore(__) and the parameter name.\n",
    "\n",
    "grid = dict()\n",
    "\n",
    "grid['model_ridge__alpha'] = np.arange(0.1,2.1,0.1)\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {
    "id": "1MgpLeo3JCPr",
    "outputId": "3e9fb7b8-6fcb-4620-8ad8-e81ebdc39481"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 49789.360\n",
      "Config: {'model_ridge__alpha': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# Create the GridSearchCV object\n",
    "\n",
    "search = GridSearchCV(estimator = final_pipeline, param_grid = grid, scoring = 'neg_mean_absolute_error',cv = 5, n_jobs= -1)\n",
    "\n",
    "# Fit GridSearchCV object to the training data\n",
    "\n",
    "results = search.fit(X_train, y_train)\n",
    "print('MAE: %.3f' % -results.best_score_)\n",
    "print('Config: %s' % results.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKlEIgQbJCPr"
   },
   "source": [
    "------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9lZXB7qZJCPr"
   },
   "source": [
    "#### Step 5: Using pipeline object to test the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {
    "id": "PgGxvzFf5JrF",
    "outputId": "19a43b36-0267-4f6b-ec99-2b1f50e33880"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 50091.26322372467\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test set using the trained model\n",
    "\n",
    "y_pred = search.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error:\", mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idA1M_uVJCPr"
   },
   "source": [
    "![link text](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/ML/Lesson_03/sklearn_pipeline_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hvo7LQPeInrT"
   },
   "source": [
    "### __Conclusion__\n",
    "\n",
    "Regression analysis is an essential method for examining and predicting variable relationships. In this lesson, we've delved into core types of regressionâ€”simple linear, multiple linear, and polynomial regression.\n",
    "\n",
    "You've acquired skills to assess model performance using metrics like MSE, RMSE, and R-squared. Furthermore, we explored regularization techniques such as Lasso, Ridge, and ElasticNet to mitigate overfitting, and learned the importance of hyperparameter tuning for optimizing model parameters to enhance results.\n",
    "\n",
    "In conclusion, this lesson has provided a comprehensive exploration of regression through theoretical insights and hands-on applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle"
      ],
      "metadata": {
        "id": "Re4wYWFzsyKP",
        "outputId": "6abaa7a6-bd1c-4d33-d094-ecaa7f7b2ff7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.6.15)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/kaggle\"\n",
        "os.makedirs(\"/content/kaggle\", exist_ok=True)\n",
        "!mv kaggle.json /content/kaggle/"
      ],
      "metadata": {
        "id": "QC12G7qkzFp7",
        "outputId": "73d6d6b6-f547-480a-b1f5-30917109869a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: cannot stat 'kaggle.json': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 600 /content/kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "DHWRMh0vzJpJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d hosammhmdali/house-price-dataset"
      ],
      "metadata": {
        "id": "cXvYoDn8z5mu",
        "outputId": "5a71a3e7-adba-4f06-969c-012e9151c2e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/hosammhmdali/house-price-dataset\n",
            "License(s): apache-2.0\n",
            "Downloading house-price-dataset.zip to /content\n",
            "  0% 0.00/400k [00:00<?, ?B/s]\n",
            "100% 400k/400k [00:00<00:00, 561MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip house-price-dataset.zip"
      ],
      "metadata": {
        "id": "9JIIF0BT0uJM",
        "outputId": "0731bb3d-f887-465d-95af-f67a410977f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  house-price-dataset.zip\n",
            "  inflating: housing.csv             \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kagglehub"
      ],
      "metadata": {
        "id": "0AQFIJOtzVoU",
        "outputId": "846dc0db-b5be-4cef-cc27-a7caa632d8ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (0.3.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kagglehub) (24.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2025.6.15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter"
      ],
      "metadata": {
        "id": "5Ub6D9QH0_Ps"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the path to the file you'd like to load\n",
        "file_path = \"\"\n",
        "\n",
        "# Load the latest version\n",
        "df = kagglehub.load_dataset(\n",
        "  KaggleDatasetAdapter.PANDAS,\n",
        "  \"hosammhmdali/house-price-dataset\",\n",
        "  file_path,\n",
        "  # Provide any additional arguments like\n",
        "  # sql_query or pandas_kwargs. See the\n",
        "  # documenation for more information:\n",
        "  # https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterpandas\n",
        ")\n",
        "\n",
        "print(\"First 5 records:\", df.head())"
      ],
      "metadata": {
        "id": "uC2d5Z-z0_Nn",
        "outputId": "e28c1ec7-694c-4933-be48-95b53f271395",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-14-2530649698.py:5: DeprecationWarning: load_dataset is deprecated and will be removed in a future version.\n",
            "  df = kagglehub.load_dataset(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Unsupported file extension: ''. Supported file extensions are: .csv, .tsv, .json, .jsonl, .xml, .parquet, .feather, .sqlite, .sqlite3, .db, .db3, .s3db, .dl3, .xls, .xlsx, .xlsm, .xlsb, .odf, .ods, .odt",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-14-2530649698.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load the latest version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m df = kagglehub.load_dataset(\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0mKaggleDatasetAdapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPANDAS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;34m\"hosammhmdali/house-price-dataset\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/kagglehub/datasets.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(adapter, handle, path, pandas_kwargs, sql_query, hf_kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;34m\"load_dataset is deprecated and will be removed in a future version.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     )\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdataset_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpandas_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpandas_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msql_query\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msql_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhf_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhf_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/kagglehub/datasets.py\u001b[0m in \u001b[0;36mdataset_load\u001b[0;34m(adapter, handle, path, pandas_kwargs, sql_query, hf_kwargs, polars_frame_type, polars_kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mkagglehub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpandas_datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             return kagglehub.pandas_datasets.load_pandas_dataset(\n\u001b[0m\u001b[1;32m    142\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpandas_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpandas_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msql_query\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msql_query\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/kagglehub/pandas_datasets.py\u001b[0m in \u001b[0;36mload_pandas_dataset\u001b[0;34m(handle, path, pandas_kwargs, sql_query)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mpandas_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpandas_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mpandas_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mfile_extension\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mread_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_read_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_extension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msql_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;31m# Now that everything has been validated, we can start downloading and processing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/kagglehub/pandas_datasets.py\u001b[0m in \u001b[0;36m_validate_read_function\u001b[0;34m(file_extension, sql_query)\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0;34mf\"Supported file extensions are: {', '.join(SUPPORTED_READ_FUNCTIONS_BY_EXTENSION.keys())}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         )\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextension_error_message\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0mread_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSUPPORTED_READ_FUNCTIONS_BY_EXTENSION\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile_extension\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unsupported file extension: ''. Supported file extensions are: .csv, .tsv, .json, .jsonl, .xml, .parquet, .feather, .sqlite, .sqlite3, .db, .db3, .s3db, .dl3, .xls, .xlsx, .xlsm, .xlsb, .odf, .ods, .odt"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_YLde4lF0_Ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VsPlettO0_Ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ejch8KgR0_H4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hulo5eay0_Fm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q788pGwAJCPp"
      },
      "source": [
        "# __Introduction to ML Pipelines__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prXug1h2rgSN"
      },
      "source": [
        "A machine learning pipeline (ML pipeline) is the systematic process of designing, developing and deploying a machine learning model. ML pipelines or ML workflows follow a series of steps that guide toward more efficient model development.\n",
        "\n",
        "The end-to-end machine learning pipeline comprises three stages:\n",
        "\n",
        "- __Data processing:__ Data scientists assemble and prepare the data that will be used to train the ML model. Phases in this stage include data collection, preprocessing, cleaning and exploration.\n",
        "- __Model development:__ Data practitioners choose or create a machine learning algorithm that fits the needs of the project. The algorithm is trained on the data from the previous step, and the resulting model is tested and validated until it is ready for use.\n",
        "- __Model deployment:__ Developers and software engineers deploy the model for real-world use, integrating it into a production environment and monitoring its performance.\n",
        "\n",
        "Machine learning workflows are a core building block for the larger discipline of machine learning operations (MLOps). Much of the process can be automated through various automated machine learning (AutoML) techniques that manage dependencies between stages and endpoints.\n",
        "\n",
        "https://www.ibm.com/think/topics/machine-learning-pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rG7g16worgSO"
      },
      "source": [
        "## __Why sklearn pipelines?__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXsPrWcaJCPp"
      },
      "source": [
        "Pipelines provide an organized approach to managing your data preprocessing and modeling code. They combine preprocessing and modeling steps into a single, streamlined process.\n",
        "\n",
        "- **Cleaner Code**: Pipelines eliminate the need to manually manage training and validation data at each preprocessing step, reducing clutter and complexity.\n",
        "- **Fewer Bugs**: By bundling steps together, pipelines minimize the risk of misapplying or forgetting a preprocessing step.\n",
        "- **Easier to Productionize**: Pipelines simplify the transition from a prototype model to a scalable, deployable solution.\n",
        "\n",
        "**Syntax**:\n",
        "```python\n",
        "class sklearn.pipeline.Pipeline(steps, *, memory=None, verbose=False)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "- A pipeline is a sequence of data transformers that can include a final predictor.\n",
        "- It lets you apply multiple preprocessing steps to your data in order, and optionally end with a predictor for modeling.\n",
        "- Each intermediate step in the pipeline must have fit and transform methods, while the final step only needs fit.\n",
        "- You can cache these transformers using the memory argument.\n",
        "- The pipeline's main goal is to combine multiple steps that can be cross-validated together and have their parameters adjusted.\n",
        "- You can set parameters for any step by using its name followed by a double underscore(__) and the parameter name.\n",
        "- You can replace any step's estimator with another estimator or remove a transformer by setting it to 'passthrough' or None."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCdpwEKfrgSO"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VC4aZ_5rgSP"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcCiCVp4rgSP"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVGDrrh7rgSP"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e_hDd8ArgSP"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHz4dWD_rgSP"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AFlMuq9rgSP"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KK2TLoSrgSP"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlNSEYhorgSP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddQRwBIUJCPp"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PN4_6toJCPp"
      },
      "source": [
        "## __Housing dataset (Regression)__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVdMCrPirgSQ"
      },
      "source": [
        "### __Dataset description__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bv4mr-BFrgSQ"
      },
      "source": [
        "The California housing dataset contains information on various socio-economic features of block groups in California. Each row in the dataset represents a single block group, and there are 20,640 observations, each with 10 attributes.\n",
        "\n",
        "The Features are as follows:\n",
        "1. Longitude: The longitude of the center of each block group in California.\n",
        "2. Latitude: The latitude of the center of each block group in California.\n",
        "3. Housing Median Age: The median age of the housing units in each block group.\n",
        "4. Total Rooms: The total number of rooms in the housing units in each block group.\n",
        "5. Total Bedrooms: The total number of bedrooms in the housing units in each block group.\n",
        "6. Population: The total population of the block group.\n",
        "7. Households: The total number of households in the block group.\n",
        "8. Median Income: The median income of the block group.\n",
        "9. Median House Value: The median value of the housing units in the block group.\n",
        "10. Ocean Proximity: The proximity of the block group to the ocean or other bodies of water."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPHQ1Rj8rgSQ"
      },
      "source": [
        "#### __Import the necessary libraries__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTzzSp9mrgSQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ot_wlncrgSR"
      },
      "source": [
        "#### __Pre-processing the dataset__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHc7WIJsJCPp",
        "outputId": "16497a75-40c4-4151-bd36-7fe74aebc5df"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'Classification/datasets/housing_with_ocean_proximity.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m housing_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassification/datasets/housing_with_ocean_proximity.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Classification/datasets/housing_with_ocean_proximity.csv'"
          ]
        }
      ],
      "source": [
        "housing_data = pd.read_csv(\"Classification/datasets/housing_with_ocean_proximity.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dY929xeLJCPp",
        "outputId": "7f8eda64-4444-4f40-ae4e-9763a1f0aff8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>longitude</th>\n",
              "      <th>latitude</th>\n",
              "      <th>housing_median_age</th>\n",
              "      <th>total_rooms</th>\n",
              "      <th>total_bedrooms</th>\n",
              "      <th>population</th>\n",
              "      <th>households</th>\n",
              "      <th>median_income</th>\n",
              "      <th>median_house_value</th>\n",
              "      <th>ocean_proximity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-122.23</td>\n",
              "      <td>37.88</td>\n",
              "      <td>41.0</td>\n",
              "      <td>880.0</td>\n",
              "      <td>129.0</td>\n",
              "      <td>322.0</td>\n",
              "      <td>126.0</td>\n",
              "      <td>8.3252</td>\n",
              "      <td>452600.0</td>\n",
              "      <td>NEAR BAY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-122.22</td>\n",
              "      <td>37.86</td>\n",
              "      <td>21.0</td>\n",
              "      <td>7099.0</td>\n",
              "      <td>1106.0</td>\n",
              "      <td>2401.0</td>\n",
              "      <td>1138.0</td>\n",
              "      <td>8.3014</td>\n",
              "      <td>358500.0</td>\n",
              "      <td>NEAR BAY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-122.24</td>\n",
              "      <td>37.85</td>\n",
              "      <td>52.0</td>\n",
              "      <td>1467.0</td>\n",
              "      <td>190.0</td>\n",
              "      <td>496.0</td>\n",
              "      <td>177.0</td>\n",
              "      <td>7.2574</td>\n",
              "      <td>352100.0</td>\n",
              "      <td>NEAR BAY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-122.25</td>\n",
              "      <td>37.85</td>\n",
              "      <td>52.0</td>\n",
              "      <td>1274.0</td>\n",
              "      <td>235.0</td>\n",
              "      <td>558.0</td>\n",
              "      <td>219.0</td>\n",
              "      <td>5.6431</td>\n",
              "      <td>341300.0</td>\n",
              "      <td>NEAR BAY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-122.25</td>\n",
              "      <td>37.85</td>\n",
              "      <td>52.0</td>\n",
              "      <td>1627.0</td>\n",
              "      <td>280.0</td>\n",
              "      <td>565.0</td>\n",
              "      <td>259.0</td>\n",
              "      <td>3.8462</td>\n",
              "      <td>342200.0</td>\n",
              "      <td>NEAR BAY</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
              "0    -122.23     37.88                41.0        880.0           129.0   \n",
              "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
              "2    -122.24     37.85                52.0       1467.0           190.0   \n",
              "3    -122.25     37.85                52.0       1274.0           235.0   \n",
              "4    -122.25     37.85                52.0       1627.0           280.0   \n",
              "\n",
              "   population  households  median_income  median_house_value ocean_proximity  \n",
              "0       322.0       126.0         8.3252            452600.0        NEAR BAY  \n",
              "1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  \n",
              "2       496.0       177.0         7.2574            352100.0        NEAR BAY  \n",
              "3       558.0       219.0         5.6431            341300.0        NEAR BAY  \n",
              "4       565.0       259.0         3.8462            342200.0        NEAR BAY  "
            ]
          },
          "execution_count": 423,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "housing_data.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFxYo8Pw5JrC"
      },
      "outputs": [],
      "source": [
        "# Separate features and target variable\n",
        "# Assuming 'median_house_value' is the target variable in your dataset\n",
        "\n",
        "X = housing_data.drop('median_house_value', axis=1)\n",
        "y = housing_data['median_house_value']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0H6lIExJCPp"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and testing sets\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=2024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmeWXusI5JrC",
        "outputId": "c5a47564-f778-4cec-bf5b-e7ed72820fbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 14448 entries, 4722 to 7816\n",
            "Data columns (total 9 columns):\n",
            " #   Column              Non-Null Count  Dtype  \n",
            "---  ------              --------------  -----  \n",
            " 0   longitude           14448 non-null  float64\n",
            " 1   latitude            14448 non-null  float64\n",
            " 2   housing_median_age  14448 non-null  float64\n",
            " 3   total_rooms         14448 non-null  float64\n",
            " 4   total_bedrooms      14286 non-null  float64\n",
            " 5   population          14448 non-null  float64\n",
            " 6   households          14448 non-null  float64\n",
            " 7   median_income       14448 non-null  float64\n",
            " 8   ocean_proximity     14448 non-null  object \n",
            "dtypes: float64(8), object(1)\n",
            "memory usage: 1.1+ MB\n",
            "None \n",
            "\n",
            "longitude               0\n",
            "latitude                0\n",
            "housing_median_age      0\n",
            "total_rooms             0\n",
            "total_bedrooms        162\n",
            "population              0\n",
            "households              0\n",
            "median_income           0\n",
            "ocean_proximity         0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Print info to check the structure of the training data\n",
        "\n",
        "print(X_train.info(), \"\\n\")\n",
        "print(X_train.isna().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKqV5G9sJCPp"
      },
      "source": [
        "#### __Implementation for sklearn pipelines__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tuxhnoYJCPp"
      },
      "source": [
        "![link text](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/ML/Lesson_03/Sklearn_pipeline_1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_nY3QOuJCPp"
      },
      "source": [
        "![link text](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/ML/Lesson_03/Sklearn_pipeline_2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdtR51LVJCPp"
      },
      "source": [
        "#### __We need to perform following preprocessing steps before building the model__|\n",
        "\n",
        "1. Missing value treatment - 162 missing values in total_bedrooms(a numeric column)\n",
        "2. Dummy variable creation for categorical data\n",
        "3. Standardization of numeric variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XnT3bUdJCPq"
      },
      "outputs": [],
      "source": [
        "# import StandardScaler for standardization and OneHotEncoder for creating dummy variables\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "# import SimpleImputer for missing value treatment\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# importing pipeline class. The Pipeline class is used to create a sequence of data processing steps.\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# importing ColumnTransformer class to apply different preprocessing steps to different subsets of features in your dataset.\n",
        "from sklearn.compose import ColumnTransformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjXlZ3znJCPq"
      },
      "source": [
        "##### __A note about ColumnTransformer__\n",
        "\n",
        "* ColumnTransformer class allows you to apply different preprocessing steps to different subsets of features in your dataset.\n",
        "* This is particularly useful when you have a mix of numerical and categorical data that require different types of preprocessing.\n",
        "* ColumnTransformer ensures that each column or group of columns gets the appropriate transformation before combining the results for further processing or modeling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLpxNEFaJCPq"
      },
      "source": [
        "### __Data preprocessing starts here:__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBrCwrleJCPq"
      },
      "source": [
        "#### Steps to perform:\n",
        "    \n",
        "#### Step 1: Let's store the names of numeric and object type variables separately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6jD0wYsJCPq"
      },
      "outputs": [],
      "source": [
        "# Set up preprocessing steps for numeric and categorical data\n",
        "\n",
        "housing_cat = X_train.select_dtypes(include='object').columns\n",
        "housing_num = X_train.select_dtypes(exclude='object').columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-CyBWuGrgSS",
        "outputId": "f088f429-d94b-45cf-c1b5-0fec0b83202a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['ocean_proximity'], dtype='object')"
            ]
          },
          "execution_count": 545,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "housing_cat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEJVs14GrgSS",
        "outputId": "8ba101e7-ef24-4e98-b895-cc023e60eca8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n",
              "       'total_bedrooms', 'population', 'households', 'median_income'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 547,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "housing_num"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkgemfJ-JCPq"
      },
      "source": [
        "------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyDCfq3lJCPq"
      },
      "source": [
        "#### Step 2: Set-up sklearn pipeline for numeric variables. We need to perform missing value imputation and then standardization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KudVlvB3JCPq"
      },
      "outputs": [],
      "source": [
        "# Numeric variables pipeline\n",
        "# Here, null values will be imputed with the median value of the dataset\n",
        "\n",
        "num_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('std_scaler', StandardScaler())\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQOz7FAPrgSd"
      },
      "outputs": [],
      "source": [
        "# Numeric variables pipeline\n",
        "# If we want to remove the rows with the null values (NOT WORKING)\n",
        "\n",
        "num_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value=np.nan)),\n",
        "    ('std_scaler', StandardScaler())\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ijvgyUGrgSd"
      },
      "outputs": [],
      "source": [
        "# Categorical variables pipeline\n",
        "\n",
        "cat_pipeline = Pipeline([\n",
        "    ('one-hot-encoder', OneHotEncoder(handle_unknown='ignore'))\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Bj7McC2JCPq"
      },
      "source": [
        "The `num_pipeline` is a pipeline that preprocesses numerical data in two steps:\n",
        "\n",
        "- Imputation: Fills missing values using the median value of each column (SimpleImputer(strategy='median')).\n",
        "- Standardization: Scales the data to have a mean of 0 and a standard deviation of 1 (StandardScaler()).\n",
        "\n",
        "This pipeline ensures consistent and streamlined preprocessing of numerical data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-ch2NFiJCPq"
      },
      "source": [
        "------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TuFRTCkJCPq"
      },
      "source": [
        "#### Step 3: Unified Data Preprocessing with Pipelines and ColumnTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQR14gk_JCPq"
      },
      "outputs": [],
      "source": [
        "# Unified preprocessing for both numeric and categorical data\n",
        "\n",
        "preprocessing = ColumnTransformer([\n",
        "    ('num', num_pipeline, housing_num),\n",
        "    ('cat', OneHotEncoder(handle_unknown='ignore'), housing_cat)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YeBHZ9flrgSe"
      },
      "outputs": [],
      "source": [
        "# Unified preprocessing for both numeric and categorical data\n",
        "\n",
        "preprocessing = ColumnTransformer([\n",
        "    ('num', num_pipeline, housing_num),\n",
        "    ('cat', cat_pipeline, housing_cat)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-A3phf1JCPq"
      },
      "source": [
        "The preprocessing step uses ColumnTransformer to apply different preprocessing pipelines to different types of data in the dataset:\n",
        "\n",
        " - Numerical Data: Applies num_pipeline to columns in housing_num, performing imputation and standardization.\n",
        " - Categorical Data: Applies OneHotEncoder to columns in housing_cat, converting categorical variables into a one-hot encoded format, ignoring unknown categories.\n",
        "\n",
        "This ensures that both numerical and categorical data are preprocessed appropriately within a single, unified framework."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaI1Qty4JCPq"
      },
      "source": [
        "------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQ0EBrXVJCPq"
      },
      "source": [
        "#### Let's check how the pipeline we have created works with the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rX9c4geYJCPq"
      },
      "outputs": [],
      "source": [
        "# applying preprocessing pipeline to train data\n",
        "\n",
        "check_train = preprocessing.fit_transform(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9t80hzsdJCPq",
        "outputId": "9eca293f-8b46-4968-b031-3b7ce1d59733"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.604388</td>\n",
              "      <td>-0.741523</td>\n",
              "      <td>1.536729</td>\n",
              "      <td>-0.620001</td>\n",
              "      <td>-0.715386</td>\n",
              "      <td>-0.790880</td>\n",
              "      <td>-0.721514</td>\n",
              "      <td>0.072853</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.798753</td>\n",
              "      <td>-0.862998</td>\n",
              "      <td>0.501005</td>\n",
              "      <td>-0.119890</td>\n",
              "      <td>-0.133529</td>\n",
              "      <td>0.218924</td>\n",
              "      <td>-0.043175</td>\n",
              "      <td>0.129574</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.845868</td>\n",
              "      <td>1.445014</td>\n",
              "      <td>-1.809456</td>\n",
              "      <td>0.736722</td>\n",
              "      <td>0.329095</td>\n",
              "      <td>0.298266</td>\n",
              "      <td>0.353837</td>\n",
              "      <td>0.825943</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1.279450</td>\n",
              "      <td>0.828299</td>\n",
              "      <td>-1.092417</td>\n",
              "      <td>-0.018491</td>\n",
              "      <td>-0.414919</td>\n",
              "      <td>-0.394171</td>\n",
              "      <td>-0.348165</td>\n",
              "      <td>3.519281</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.237318</td>\n",
              "      <td>-1.381600</td>\n",
              "      <td>1.457057</td>\n",
              "      <td>-0.847115</td>\n",
              "      <td>-0.739232</td>\n",
              "      <td>-0.702522</td>\n",
              "      <td>-0.747807</td>\n",
              "      <td>-0.986694</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         0         1         2         3         4         5         6   \\\n",
              "0  0.604388 -0.741523  1.536729 -0.620001 -0.715386 -0.790880 -0.721514   \n",
              "1  0.798753 -0.862998  0.501005 -0.119890 -0.133529  0.218924 -0.043175   \n",
              "2 -0.845868  1.445014 -1.809456  0.736722  0.329095  0.298266  0.353837   \n",
              "3 -1.279450  0.828299 -1.092417 -0.018491 -0.414919 -0.394171 -0.348165   \n",
              "4  1.237318 -1.381600  1.457057 -0.847115 -0.739232 -0.702522 -0.747807   \n",
              "\n",
              "         7    8    9    10   11   12  \n",
              "0  0.072853  1.0  0.0  0.0  0.0  0.0  \n",
              "1  0.129574  1.0  0.0  0.0  0.0  0.0  \n",
              "2  0.825943  0.0  1.0  0.0  0.0  0.0  \n",
              "3  3.519281  0.0  0.0  0.0  1.0  0.0  \n",
              "4 -0.986694  0.0  0.0  0.0  0.0  1.0  "
            ]
          },
          "execution_count": 565,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# converting array to dataframe to have a better look at it\n",
        "\n",
        "check_train_df = pd.DataFrame(check_train)\n",
        "check_train_df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCQZ6iNgJCPq",
        "outputId": "f5d2ea66-b060-4fb9-e684-c9aaf70f836e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0       0\n",
              "1       0\n",
              "2       0\n",
              "3       0\n",
              "4     162\n",
              "5       0\n",
              "6       0\n",
              "7       0\n",
              "8       0\n",
              "9       0\n",
              "10      0\n",
              "11      0\n",
              "12      0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 567,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# No NULL values found after pre-processing\n",
        "\n",
        "check_train_df.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkLLRW9-JCPq"
      },
      "source": [
        "#### __Observation:__\n",
        "\n",
        "1. Good to see all the missing values are treated\n",
        "2. numeric variables are standardised\n",
        "3. ocean_proximity is converted to dummy variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_03hcT-RJCPr"
      },
      "source": [
        "------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "156uciNUJCPr"
      },
      "source": [
        "#### Step 4: Building Linear regression model and hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XD6Lk-FQJCPr"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_absolute_error\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDKVzBGqJCPr"
      },
      "outputs": [],
      "source": [
        "# Create an instance of the Ridge regression model\n",
        "\n",
        "model = Ridge()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lk6a6jk1JCPr"
      },
      "outputs": [],
      "source": [
        "# Build a Ridge regression model within a complete pipeline\n",
        "\n",
        "final_pipeline = Pipeline([\n",
        "    ('preprocessing', preprocessing),\n",
        "    ('model_ridge', model)\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhILfy3zJCPr",
        "outputId": "dbf32d9d-1c3e-42fa-abd4-e76ef25ec743"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'model_ridge__alpha': array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. , 1.1, 1.2, 1.3,\n",
              "        1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2. ])}"
            ]
          },
          "execution_count": 535,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define the grid of hyperparameters to search\n",
        "# Note: You can set parameters for any step by using its name followed by a double underscore(__) and the parameter name.\n",
        "\n",
        "grid = dict()\n",
        "\n",
        "grid['model_ridge__alpha'] = np.arange(0.1,2.1,0.1)\n",
        "grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MgpLeo3JCPr",
        "outputId": "3e9fb7b8-6fcb-4620-8ad8-e81ebdc39481"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MAE: 49789.360\n",
            "Config: {'model_ridge__alpha': 0.1}\n"
          ]
        }
      ],
      "source": [
        "# Create the GridSearchCV object\n",
        "\n",
        "search = GridSearchCV(estimator = final_pipeline, param_grid = grid, scoring = 'neg_mean_absolute_error',cv = 5, n_jobs= -1)\n",
        "\n",
        "# Fit GridSearchCV object to the training data\n",
        "\n",
        "results = search.fit(X_train, y_train)\n",
        "print('MAE: %.3f' % -results.best_score_)\n",
        "print('Config: %s' % results.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKlEIgQbJCPr"
      },
      "source": [
        "------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lZXB7qZJCPr"
      },
      "source": [
        "#### Step 5: Using pipeline object to test the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgGxvzFf5JrF",
        "outputId": "19a43b36-0267-4f6b-ec99-2b1f50e33880"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Absolute Error: 50091.26322372467\n"
          ]
        }
      ],
      "source": [
        "# Predict on the test set using the trained model\n",
        "\n",
        "y_pred = search.predict(X_test)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(\"Mean Absolute Error:\", mae)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idA1M_uVJCPr"
      },
      "source": [
        "![link text](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/ML/Lesson_03/sklearn_pipeline_3.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvo7LQPeInrT"
      },
      "source": [
        "### __Conclusion__\n",
        "\n",
        "Regression analysis is an essential method for examining and predicting variable relationships. In this lesson, we've delved into core types of regressionsimple linear, multiple linear, and polynomial regression.\n",
        "\n",
        "You've acquired skills to assess model performance using metrics like MSE, RMSE, and R-squared. Furthermore, we explored regularization techniques such as Lasso, Ridge, and ElasticNet to mitigate overfitting, and learned the importance of hyperparameter tuning for optimizing model parameters to enhance results.\n",
        "\n",
        "In conclusion, this lesson has provided a comprehensive exploration of regression through theoretical insights and hands-on applications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WC3X7e4CrgSf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
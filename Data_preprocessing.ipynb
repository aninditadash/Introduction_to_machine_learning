{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e27070af-360d-4664-8d9a-ae82595a1b52",
   "metadata": {},
   "source": [
    "# __Data Preprocessing__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac659043-6058-4fe3-8411-b5d862a4d63e",
   "metadata": {},
   "source": [
    "## __Train-Test split__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b867fe3-00d7-484d-9bc0-91660af9e0fd",
   "metadata": {},
   "source": [
    "Train test split is a model validation procedure that allows you to simulate how a model would perform on new/unseen data by splitting a dataset into a training set and a testing set. \n",
    "\n",
    "- The training set is data used to train the model, and the testing set data (which is new to the model) is used to test the modelâ€™s performance and accuracy.\n",
    "- A train test split can also involve splitting data into a validation set, which is data used to fine-tune hyperparameters and optimize the model during the training process.\n",
    "\n",
    "<img src=\"https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/1_train-test-split_0.jpg\" width=700 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2330cee3-01dd-4b22-b797-a0749e267eba",
   "metadata": {},
   "source": [
    "#### __Methods for Splitting Data in a Train Test Split__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bad06d9-9f57-415a-9610-c570a788ddf0",
   "metadata": {},
   "source": [
    "Some common methods of splitting data in a train test split:\n",
    "\n",
    "1. __Random Splitting:__ involves randomly shuffling data and splitting it into training and testing sets based on given percentages (like 75% training and 25% testing).\n",
    "   \n",
    "2. __Stratified Splitting:__ divides a dataset in a way that preserves its proportion of classes or categories. This creates training and testing sets with class proportions representative of the original dataset. Using stratified splitting can prevent model bias, and is most effective for imbalanced datasets. Use `stratify` parameter in the `train_test_split()` method.\n",
    "\n",
    "3. __Time-Based Splitting:__ involves organizing data in a set by points in time, ensuring past data is in the training set and future or later data is in the testing set. Splitting data based on time works to simulate real-world scenarios (for example, predicting future financial or market trends) and allows for time series analysis on time series datasets. However, one drawback to time-based splitting is that it may not fully capture trends for non-stationary data (data that continually changes over time). In scikit-learn, time series data can be split into training and testing sets by using the `TimeSeriesSplit()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982d4a51-1104-4a06-a735-e8bfa6e66bb3",
   "metadata": {},
   "source": [
    "## __Data Standardization__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970a9eb4-f597-46a3-8b47-29f21da5333c",
   "metadata": {},
   "source": [
    "Data standardization comes into the picture when features of the input data set have large differences between their ranges, or simply when they are measured in different units. It converts data into a standard, uniform format, making it consistent across different data sets and easier to understand for machine learning or statistical models. \n",
    "\n",
    "- Standardizing data can enhance data quality and accuracy, which helps users make reliable data-driven decisions.\n",
    "- Z-score normalization, or standardization, is one of the most popular methods to standardize data.\n",
    "- With this method, data is transformed to have a mean of 0 and a standard deviation of 1, giving all data points the same scale.\n",
    "- We can use the `StandardScaler()` method from scikit-learn.\n",
    "- `StandardScaler()` provides the 3 methods fit(), transform(), and fit_transform().\n",
    "- `fit()` method - takes the dataset we aim to standardize as an argument and computes its mean and standard deviation.\n",
    "- `transform()` method - applies the scaling performed using the `.fit()` method to every feature value.\n",
    "- `fit_transform()` method - does both `.fit()` and `.transform()`. Has more computational efficiency as it combines two methods into one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a67859-43e6-4780-9844-32727f9351ab",
   "metadata": {},
   "source": [
    "### __Should we perform `.fit_transform()` before or after the split of training and test data?__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24bf112-3442-4b8f-b1b4-51d95ff3f0e5",
   "metadata": {},
   "source": [
    "Normalization / Standardization should be done after splitting the data into train and test sets. The reason is to avoid any data leakage. This is also applicable to `CountVectorizer()` where it counts the no. of words in a text message.\n",
    "\n",
    "__Data Leakage:__ Data leakage happens when information from outside the training set is used to create the model. This additional information can allow the model to learn or know something that it otherwise would not know and in turn invalidate the estimated performance of the model being constructed.\n",
    "\n",
    "The testing data points represent real-world data. Feature normalization (or data standardisation) of the explanatory (or predictor) variables is a technique used to center and normalise the data by subtracting the mean and dividing by the standard deviation. If you take the mean and variance of the whole dataset, you will be introducing future information into the explanatory variables (i.e. the mean and std. deviation). \n",
    "\n",
    "- We perform standardisation as `fit_transform()` on the training set and `transform()` on the testing set.\n",
    "- `fit_transform()` on the train data standardises it and calculates the mean and standard deviation for the train data, `transform()` is used on the test data which means we will apply the metrics calculated from the train set onto the test set. We do so to prevent data leakage, i.e. learning something new from the test data, in order to accurately test the selected model.\n",
    "- Using the `fit_transform` method on the entire dataset could cause the model to overperform, as it would have prior knowledge of the test data vocabulary, leading to an unrealistic assessment of its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15bb726-c24f-4696-88e3-2dcc16d38985",
   "metadata": {},
   "source": [
    "## __Missing data imputation__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cefe9b-38ac-433d-94c7-36af797f5c6f",
   "metadata": {},
   "source": [
    "Missing data imputation is a technique used to fill in missing values within a dataset, preventing potential issues with analysis or model training. It involves replacing missing values with estimated values based on the existing data, ensuring the dataset is complete and usable for further analysis.\n",
    "\n",
    "- __Univariate Imputation:__ This method focuses on a single variable, using the mean, median, or mode of the non-missing values to fill in the missing values for that specific variable. \n",
    "- __Multivariate Imputation:__ This method considers multiple variables to estimate the missing values. It often involves using regression models or other statistical methods to predict the missing values based on the relationships between the variables. \n",
    "- __Multiple Imputation:__ This technique creates multiple imputed datasets by generating different estimates for the missing values. This allows for incorporating uncertainty about the true values, as analysis can be performed separately on each imputed dataset and results can be pooled. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601d5d1a-d158-4dfd-a770-1f5afe8b84fb",
   "metadata": {},
   "source": [
    "#### __Common Imputation Techniques:__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa49e448-f142-4c7e-8c36-3264d89bbf30",
   "metadata": {},
   "source": [
    "1. __Mean, Median, and Mode Imputation:__ Replacing missing values with the average, middle value, or most frequent value, respectively. \n",
    "2. __Constant Value Imputation:__ Replacing missing values with a predetermined constant, which can be a specific value or a value representing an \"unknown\" or \"missing\" category. \n",
    "3. __Regression Imputation:__ Using regression models to predict missing values based on other available variables. \n",
    "4. __K-Nearest Neighbors Imputation:__ Replacing missing values with the average of the values from the nearest neighbors in the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5e94f3-f844-421a-8e60-d54c1852d467",
   "metadata": {},
   "source": [
    "## __Dealing with Missing Values__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff48e4a-89b4-422f-a5db-abd92604f9aa",
   "metadata": {},
   "source": [
    "Customer Lifetime Value is the total monetary worth the customer has to the business, over the course of its business-customer relationship. Below are different missing value imputation techniques by analyzing simulated customer lifetime value data. \n",
    "\n",
    "- Check the number of null values\n",
    "- Dropping Null Values\n",
    "- Mean/Median/Mode Imputation\n",
    "- Multiple Imputation using Regression\n",
    "- Imputation using Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b95fba1-a39c-4623-83c5-19b7eda2837c",
   "metadata": {},
   "source": [
    "### __Import the Libraries__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8117ebf3-cd32-4070-aaef-a9e6fd38d744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, KNNImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "07d91508-ac94-40c2-9a72-e441c5c8a5cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>income</th>\n",
       "      <th>days_on_platform</th>\n",
       "      <th>city</th>\n",
       "      <th>purchases</th>\n",
       "      <th>lifetime_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Male</td>\n",
       "      <td>126895</td>\n",
       "      <td>14.0</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Male</td>\n",
       "      <td>161474</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Tokyo</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>24.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>104723</td>\n",
       "      <td>34.0</td>\n",
       "      <td>London</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>29.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>43791</td>\n",
       "      <td>28.0</td>\n",
       "      <td>London</td>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>18.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>132181</td>\n",
       "      <td>26.0</td>\n",
       "      <td>London</td>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  id   age  gender  income  days_on_platform           city  \\\n",
       "0           0   0   NaN    Male  126895              14.0  San Francisco   \n",
       "1           1   1   NaN    Male  161474              14.0          Tokyo   \n",
       "2           2   2  24.0    Male  104723              34.0         London   \n",
       "3           3   3  29.0    Male   43791              28.0         London   \n",
       "4           4   4  18.0  Female  132181              26.0         London   \n",
       "\n",
       "   purchases  lifetime_value  \n",
       "0          0               0  \n",
       "1          0               0  \n",
       "2          1              20  \n",
       "3          2              40  \n",
       "4          2              40  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('datasets/clv_data.csv')\n",
    "\n",
    "# Since calculation of customer lifetime value is extremely complex, we will use a simple assumption for this example\n",
    "df['lifetime_value'] = df['purchases'] * 20\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec11227c-4cf0-42c8-a4b5-f0a2455e44fc",
   "metadata": {},
   "source": [
    "### __Checking null values__\n",
    "\n",
    "The first step in any data analysis or ML model is to check null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb635ff2-f41e-4749-b86a-7429489ea896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0             0\n",
       "id                     0\n",
       "age                 2446\n",
       "gender                 0\n",
       "income                 0\n",
       "days_on_platform     141\n",
       "city                   0\n",
       "purchases              0\n",
       "lifetime_value         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4792438-bdad-46a6-8051-a5882d652677",
   "metadata": {},
   "source": [
    "### __Dropping null values__\n",
    "\n",
    "Dropping nulls is the quickest and easiest method to remove null/missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbfbd853-ed7a-4b24-9ba1-e192b9fa6871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0          0\n",
       "id                  0\n",
       "age                 0\n",
       "gender              0\n",
       "income              0\n",
       "days_on_platform    0\n",
       "city                0\n",
       "purchases           0\n",
       "lifetime_value      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_df = df.copy()\n",
    "\n",
    "drop_df = drop_df.dropna()\n",
    "drop_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ea9469-76e8-4b4f-b626-39f3a64fe009",
   "metadata": {},
   "source": [
    "#### __Divide dataset into training and testing sets__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42dbb1f0-40b1-435a-a147-d6c593fd78ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Feature and target variables\n",
    "X = drop_df[['age', 'gender', 'days_on_platform', 'income']]\n",
    "y = drop_df['lifetime_value']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c102f10-94f1-4b95-a185-4de423fdb660",
   "metadata": {},
   "source": [
    "### __Mean/Median/Mode Imputation for missing values__\n",
    "\n",
    "The next is mean/median/mode imputation which can be used to fill the null/missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bddfe962-ab98-4bd1-92a4-b5f5a2589b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_df = df.copy()\n",
    "\n",
    "# Create Feature and target variables\n",
    "X = m_df[['age', 'gender', 'days_on_platform', 'income']]\n",
    "y = m_df['lifetime_value']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "19ee3332-dfbc-47af-b1bb-da697a44a2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean/median/mode of the training set and use it to fill the null values in testing set to avoid data leakage\n",
    "\n",
    "X_train.fillna({'age': np.mean(X_train['age']), 'days_on_platform': np.mean(X_train['days_on_platform'])}, inplace=True)\n",
    "X_test.fillna({'age': np.mean(X_train['age']), 'days_on_platform': np.mean(X_train['days_on_platform'])}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3d2e15-e17f-4ef7-a4a4-22c3d16ed334",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed993658-97e3-4680-9591-56f5b841f5d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f159d9-9a6b-4520-b198-15a58d98489d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567deca7-0a49-4431-a9b1-0101bf0df9e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49e9f9f-5785-4577-b99f-3529089084f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e566af6-deba-4cf4-abe0-aece60140885",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4163882-e228-4e36-8825-dc2c3aa08ed2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b237497d-736e-4365-a2ff-900277d08fcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e5637c-cacd-499b-ac6e-d5d7c5507792",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
